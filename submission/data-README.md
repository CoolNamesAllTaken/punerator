NOTE: this zip file contains no files of interest, other than this README file.  Please download the items specified from the links below, and place them in their relevant folders.  Data for this program consists of corpora in the `corpora/` folder and models in the `models/` folder.  `corpora/`, `models/`, `thesaurus_api/`, and the top level files in code.zip should all be in the same directory.

## corpora/
Add EnglishText.txt to the corpora folder for bigram retraining.  Not required for running the program if bigram cost has already been trained (bigram and unigram cost functions are saved in models/).  Corpora can be downloaded [here](https://drive.google.com/drive/folders/1-M4lIEzhLOQlofToD6S7KpCwlVTBR7xu?usp=sharing)

## models/
Add the GoogleNews pre-trained word2vec binary to the models/ folder.  The file can be found [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing).  Make sure to uncompress the file before adding it to the folder (extension should be .bin, not .bin.gz).  Alternatively, if you do not want to train the bigram and unigram cost models, download the files from [this link](https://drive.google.com/drive/folders/1OwQlf4jGrl4hz_o0JclpfT3Mlp0BJLWX?usp=sharing) and move them into the models/ folder.